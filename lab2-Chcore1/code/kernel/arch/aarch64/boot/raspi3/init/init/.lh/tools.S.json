{
    "sourceFile": "tools.S",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1697699567244,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1697699567244,
            "name": "Commit-0",
            "content": "/*\n * Copyright (c) 2023 Institute of Parallel And Distributed Systems (IPADS), Shanghai Jiao Tong University (SJTU)\n * Licensed under the Mulan PSL v2.\n * You can use this software according to the terms and conditions of the Mulan PSL v2.\n * You may obtain a copy of Mulan PSL v2 at:\n *     http://license.coscl.org.cn/MulanPSL2\n * THIS SOFTWARE IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY OR FIT FOR A PARTICULAR\n * PURPOSE.\n * See the Mulan PSL v2 for more details.\n */\n\n// Copyright 2016 The Fuchsia Authors\n// Copyright (c) 2014, Google Inc. All rights reserved\n//\n// Use of this source code is governed by a MIT-style\n// license that can be found in the LICENSE file or at\n// https://opensource.org/licenses/MIT\n\n/*\n   MIT License\n\n   Copyright (c) 2018 Sergey Matyukevich\n\n   Permission is hereby granted, free of charge, to any person obtaining a copy\n   of this software and associated documentation files (the \"Software\"), to deal\n   in the Software without restriction, including without limitation the rights\n   to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n   copies of the Software, and to permit persons to whom the Software is\n   furnished to do so, subject to the following conditions:\n\n   The above copyright notice and this permission notice shall be included in all\n   copies or substantial portions of the Software.\n\n   THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n   IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n   FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n   AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n   LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n   OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\n   SOFTWARE.\n*/\n\n#include <common/asm.h>\n#include <arch/machine/registers.h>\n\n#define CURRENTEL_EL1           (0b01 << 2)\n#define CURRENTEL_EL2           (0b10 << 2)\n\n#define CPACR_EL1_FPEN          (0b11 << 20)\n#define ID_AA64PFR0_EL1_GIC     (0b1111 << 24)\n\n#define CNTHCTL_EL2_EL1PCEN     (1 << 1)\n#define CNTHCTL_EL2_EL1PCTEN    (1 << 0)\n#define CPTR_EL2_RES1           0x33ff\n#define HCR_EL2_RW              (1 << 31)\n#define ICC_SRE_EL2_SRE         (1 << 0)\n#define ICC_SRE_EL2_ENABLE      (1 << 3)\n\n#define SCR_EL3_HCE             (1 << 8)\n#define SCR_EL3_NS              (1 << 0)\n#define SCR_EL3_RW              (1 << 10)\n\n#define SPSR_ELX_DAIF           (0b1111 << 6)\n#define SPSR_ELX_EL1H           (0b0101)\n\n#define ICH_HCR_EL2             S3_4_C12_C11_0\n#define ICC_SRE_EL2             S3_4_C12_C9_5\n\nBEGIN_FUNC(arm64_elX_to_el1)\n\t/* LAB 1 TODO 1 BEGIN */\n\t/* BLANK BEGIN */\n\tmrs x9, CurrentEL\n\t/* BLANK END */\n\t/* LAB 1 TODO 1 END */\n\n\t// Check the current exception level.\n\tcmp x9, CURRENTEL_EL1\n\tbeq .Ltarget\n\tcmp x9, CURRENTEL_EL2\n\tbeq .Lin_el2\n\t// Otherwise, we are in EL3.\n\n\t// Set EL2 to 64bit and enable the HVC instruction.\n\tmrs x9, scr_el3\n\tmov x10, SCR_EL3_NS | SCR_EL3_HCE | SCR_EL3_RW\n\torr x9, x9, x10\n\tmsr scr_el3, x9\n\n\t// Set the return address and exception level.\n\t/* LAB 1 TODO 2 BEGIN */\n\t/* BLANK BEGIN */\n\tadr x9, .Ltarget\n\tmsr elr_el3, x9\n\tmov x9, SPSR_ELX_DAIF | SPSR_ELX_EL1H\n\tmsr spsr_el3, x9\n\t/* BLANK END */\n\t/* LAB 1 TODO 2 END */\n\n.Lin_el2:\n\t// Disable EL1 timer traps and the timer offset.\n\tmrs x9, cnthctl_el2\n\torr x9, x9, CNTHCTL_EL2_EL1PCEN | CNTHCTL_EL2_EL1PCTEN\n\tmsr cnthctl_el2, x9\n\tmsr cntvoff_el2, xzr\n\n\t// Disable stage 2 translations.\n\tmsr vttbr_el2, xzr\n\n\t// Disable EL2 coprocessor traps.\n\tmov x9, CPTR_EL2_RES1\n\tmsr cptr_el2, x9\n\n\t// Disable EL1 FPU traps.\n\tmov x9, CPACR_EL1_FPEN\n\tmsr cpacr_el1, x9\n\n\t// Check whether the GIC system registers are supported.\n\tmrs x9, id_aa64pfr0_el1\n\tand x9, x9, ID_AA64PFR0_EL1_GIC\n\tcbz x9, .Lno_gic_sr\n\n\t// Enable the GIC system registers in EL2, and allow their use in EL1.\n\tmrs x9, ICC_SRE_EL2\n\tmov x10, ICC_SRE_EL2_ENABLE | ICC_SRE_EL2_SRE\n\torr x9, x9, x10\n\tmsr ICC_SRE_EL2, x9\n\n\t// Disable the GIC virtual CPU interface.\n\tmsr ICH_HCR_EL2, xzr\n\n.Lno_gic_sr:\n\t// Set EL1 to 64bit.\n\tmov x9, HCR_EL2_RW\n\tmsr hcr_el2, x9\n\n\t// Set the return address and exception level.\n\tadr x9, .Ltarget\n\tmsr elr_el2, x9\n\tmov x9, SPSR_ELX_DAIF | SPSR_ELX_EL1H\n\tmsr spsr_el2, x9\n\n\tisb\n\teret\n\n.Ltarget:\n\tret\nEND_FUNC(arm64_elX_to_el1)\n\n// See https://developer.arm.com/documentation/den0024/a/Caches/Cache-maintenance\nBEGIN_FUNC(invalidate_cache_all)\n\tmrs     x0, clidr_el1\n\tand     w3, w0, #0x07000000     // get 2x level of coherence\n\tlsr     w3, w3, #23\n\tcbz     w3, .Lfinished_inv_cache\n\tmov     w10, #0                 // w10 = 2x cache level\n\tmov     w8, #1                  // w8 = constant 1\n.Lloop1_inv_cache:\n\tadd     w2, w10, w10, lsr #1    // calculate 3x cache level\n\tlsr     w1, w0, w2              // extract 3 bit cache type for this level\n\tand     w1, w1, #0x7\n\tcmp     w1, #2\n\tb.lt    .Lskip_inv_cache        // no data or unified cache at this level\n\tmsr     csselr_el1, x10         // select this cache level\n\tisb                             // synchronize change to csselr\n\tmrs     x1, ccsidr_el1          // w1 = ccsidr\n\tand     w2, w1, #7              // w2 = log2(line len) - 4\n\tadd     w2, w2, #4              // w2 = log2(line len)\n\tubfx    w4, w1, #3, #10         // w4 = max way number, right aligned\n\tclz     w5, w4                  // w5 = 32 - log2(ways), bit position of way in DC operand\n\tlsl     w9, w4, w5              // w9 = max way number, aligned to position in DC operand\n\tlsl     w12, w8, w5             // w12 = amount to decrement way number per iteration\n\n.Lloop2_inv_cache:\n\tubfx    w7, w1, #13, #15        // w7 = max set number, right aligned\n\tlsl     w7, w7, w2              // w7 = max set number, aligned to position in DC operand\n\tlsl     w13, w8, w2             // w13 = amount to decrement set number per iteration\n.Lloop3_inv_cache:\n\torr     w11, w10, w9            // w11 = combine way number and cache number\n\torr     w11, w11, w7            //       and set number for DC operand\n\tdc      isw, x11                // data cache op\n\tsubs    w7, w7, w13             // decrement set number\n\tb.ge    .Lloop3_inv_cache\n\n\tsubs    x9, x9, x12             // decrement way number\n\tb.ge    .Lloop2_inv_cache\n.Lskip_inv_cache:\n\tadd     w10, w10, #2            // increment 2x cache level\n\tcmp     w3, w10\n\tdsb     sy                      // ensure completetion of previous cache maintainance instructions\n\tb.gt    .Lloop1_inv_cache\n.Lfinished_inv_cache:\n\n\t// dump the instruction cache as well\n\tic      iallu\n\tisb\n\tret\nEND_FUNC(invalidate_cache_all)\n\n.extern boot_ttbr0_l0\n.extern boot_ttbr1_l0\n\n\n/* DEVICE_nGnRnE */\n#define MMU_MAIR_ATTR0\t\t(0x00 << (8 * 0))\n\n/* DEVICE_nGnRE */\n#define MMU_MAIR_ATTR1\t\t(0x04 << (8 * 1))\n\n/* DEVICE_GRE */\n#define MMU_MAIR_ATTR2\t\t(0x0c << (8 * 2))\n\n/* NORMAL_NC */\n#define MMU_MAIR_ATTR3          (0x44 << (8 * 3))\n\n/* NORMAL */\n#define MMU_MAIR_ATTR4          (0xff << (8 * 4))\n\n/*\n * Enable cached page table walks:\n * inner/outer (IRGN/ORGN): write-back + write-allocate\n */\n#define MMU_TCR_TG1_4k \t\t\t(0 << 14)\n#define MMU_TCR_SH1_INNER_SH\t(3 << 28)\n#define MMU_TCR_ORGN1_WBA\t\t(1 << 26)\n#define MMU_TCR_IRGN1_WBA\t\t(1 << 24)\n#define MMU_TCR_T1SZ\t\t\t((64 - 48) << 16) /* 48-bit  */\n#define MMU_TCR_FLAGS1\t\t\t(MMU_TCR_TG1_4k | MMU_TCR_SH1_INNER_SH | \\\n \t\t\t\t\t\tMMU_TCR_ORGN1_WBA | MMU_TCR_IRGN1_WBA | MMU_TCR_T1SZ)\n\n#define MMU_TCR_TG0_4k \t\t\t(0 << 30)\n#define MMU_TCR_SH0_INNER_SH\t(3 << 12)\n#define MMU_TCR_ORGN0_WBA\t\t(1 << 10)\n#define MMU_TCR_IRGN0_WBA\t\t(1 << 8)\n#define MMU_TCR_T0SZ\t\t\t((64 - 48) << 0) /* 48-bit */\n#define MMU_TCR_FLAGS0\t\t\t(MMU_TCR_TG0_4k | MMU_TCR_SH0_INNER_SH | \\\n \t\t\t\t\t\tMMU_TCR_ORGN0_WBA | MMU_TCR_IRGN0_WBA | MMU_TCR_T0SZ)\n#define MMU_TCR_IPS \t\t\t(0b101 << 32) /* 48-bit */\n#define MMU_TCR_AS\t\t\t\t(1 << 36)\n\n\nBEGIN_FUNC(el1_mmu_activate)\n\tstp     x29, x30, [sp, #-16]!\n\tmov     x29, sp\n\n\tbl\tinvalidate_cache_all\n\n\t/* Invalidate TLB */\n\ttlbi    vmalle1is\n\tisb\n\tdsb     sy\n\n\t/* Initialize Memory Attribute Indirection Register */\n\tldr \tx8, =MMU_MAIR_ATTR0 | MMU_MAIR_ATTR1 | MMU_MAIR_ATTR2 | MMU_MAIR_ATTR3 | MMU_MAIR_ATTR4\n\tmsr     mair_el1, x8\n\n\t/* Initialize TCR_EL1 */\n\t/* set cacheable attributes on translation walk */\n\t/* (SMP extensions) non-shareable, inner write-back write-allocate */\n\tldr  \tx8, =MMU_TCR_FLAGS1 | MMU_TCR_FLAGS0 | MMU_TCR_IPS | MMU_TCR_AS\n\tmsr     tcr_el1, x8\n\tisb\n\n\t/* Write ttbr with phys addr of the translation table */\n\tadrp    x8, boot_ttbr0_l0\n\tmsr     ttbr0_el1, x8\n\tadrp    x8, boot_ttbr1_l0\n\tmsr     ttbr1_el1, x8\n\tisb\n\n\tmrs     x8, sctlr_el1\n\t/* Enable MMU */\n\t/* LAB 1 TODO 4 BEGIN */\n\t/* BLANK BEGIN */\n\torr     x8, x8, #SCTLR_EL1_M\n\t/* BLANK END */\n\t/* LAB 1 TODO 4 END */\n\t/* Disable alignment checking */\n\tbic     x8, x8, #SCTLR_EL1_A\n\tbic     x8, x8, #SCTLR_EL1_SA0\n\tbic     x8, x8, #SCTLR_EL1_SA\n\torr     x8, x8, #SCTLR_EL1_nAA\n\t/* Data accesses Cacheable */\n\torr     x8, x8, #SCTLR_EL1_C\n\t/* Instruction access Cacheable */\n\torr     x8, x8, #SCTLR_EL1_I\n\tmsr     sctlr_el1, x8\n\n\tldp     x29, x30, [sp], #16\n\tret\nEND_FUNC(el1_mmu_activate)\n\n\n/* These simple function is written by ChCorers */\nBEGIN_FUNC(early_put32)\n\tstr w1, [x0]\n\tret\nEND_FUNC(early_put32)\n\nBEGIN_FUNC(early_get32)\n\tldr w0, [x0]\n\tret\nEND_FUNC(early_get32)\n\nBEGIN_FUNC(delay)\n\tsubs x0, x0, #1\n\tbne delay\n\tret\nEND_FUNC(delay)\n"
        }
    ]
}