{
    "sourceFile": "kernel/mm/kmalloc.c",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1699511679139,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1699511679139,
            "name": "Commit-0",
            "content": "/*\n * Copyright (c) 2023 Institute of Parallel And Distributed Systems (IPADS), Shanghai Jiao Tong University (SJTU)\n * Licensed under the Mulan PSL v2.\n * You can use this software according to the terms and conditions of the Mulan PSL v2.\n * You may obtain a copy of Mulan PSL v2 at:\n *     http://license.coscl.org.cn/MulanPSL2\n * THIS SOFTWARE IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY OR FIT FOR A PARTICULAR\n * PURPOSE.\n * See the Mulan PSL v2 for more details.\n */\n\n#include <common/debug.h>\n#include <common/types.h>\n#include <common/macro.h>\n#include <common/errno.h>\n#include <common/util.h>\n#include <common/kprint.h>\n#include <lib/mem_usage_info_tool.h>\n\n#include <mm/slab.h>\n#include <mm/buddy.h>\n\n#define SLAB_MAX_SIZE (1UL << SLAB_MAX_ORDER)\n#define ZERO_SIZE_PTR ((void *)(-1UL))\n\nvoid *_get_pages(int order, bool is_record)\n{\n        struct page *page = NULL;\n        int i;\n        void *addr;\n\n        /* Try to get continous physical memory pages from one physmem pool. */\n        for (i = 0; i < physmem_map_num; ++i) {\n                page = buddy_get_pages(&global_mem[i], order);\n                if (page)\n                        break;\n        }\n\n        if (unlikely(!page)) {\n                kwarn(\"[OOM] Cannot get page from any memory pool!\\n\");\n                return NULL;\n        }\n\n\taddr = page_to_virt(page);\n#if ENABLE_MEMORY_USAGE_COLLECTING == ON\n        if(is_record && collecting_switch) {\n                record_mem_usage(BUDDY_PAGE_SIZE << order, addr);\n\t}\n#endif\n        return addr;\n}\n\nvoid *get_pages(int order)\n{\n        return _get_pages(order, true);\n}\n\nvoid _free_pages(void *addr, bool is_revoke_record)\n{\n        struct page *page;\n\n        page = virt_to_page(addr);\n        if (!page) {\n                kdebug(\"invalid page in %s\", __func__);\n                return;\n        }\n#if ENABLE_MEMORY_USAGE_COLLECTING == ON\n        if (collecting_switch && is_revoke_record) {\n                revoke_mem_usage(addr);\n\t}\n#endif\n        buddy_free_pages(page->pool, page);\n}\n\nvoid free_pages(void *addr)\n{\n        _free_pages(addr, true);\n}\n\nvoid free_pages_without_record(void *addr)\n{\n        _free_pages(addr, false);\n}\n\nstatic int size_to_page_order(unsigned long size)\n{\n        unsigned long order;\n        unsigned long pg_num;\n        unsigned long tmp;\n\n        order = 0;\n        pg_num = ROUND_UP(size, BUDDY_PAGE_SIZE) / BUDDY_PAGE_SIZE;\n        tmp = pg_num;\n\n        while (tmp > 1) {\n                tmp >>= 1;\n                order += 1;\n        }\n\n        if (pg_num > (1 << order))\n                order += 1;\n\n        return (int)order;\n}\n\n/* Currently, BUG_ON no available memory. */\nvoid *_kmalloc(size_t size, bool is_record, size_t *real_size)\n{\n        void *addr;\n        int order;\n\n        if (unlikely(size == 0))\n                return ZERO_SIZE_PTR;\n\n        if (size <= SLAB_MAX_SIZE) {\n                /* LAB 2 TODO 3 BEGIN */\n                /* Step 1: Allocate in slab for small requests. */\n                /* BLANK BEGIN */\n\t\taddr = alloc_in_slab(size, real_size);\n                /* BLANK END */\n#if ENABLE_MEMORY_USAGE_COLLECTING == ON\n                if(is_record && collecting_switch) {\n                        record_mem_usage(*real_size, addr);\n\t\t}\n#endif\n        } else {\n                /* Step 2: Allocate in buddy for large requests. */\n                /* BLANK BEGIN */\n                order = size_to_page_order(size);\n\t\taddr = get_pages(order);\n                /* BLANK END */\n                /* LAB 2 TODO 3 END */\n        }\n\n        BUG_ON(!addr);\n        return addr;\n}\n\nvoid *kmalloc(size_t size)\n{\n        size_t real_size;\n        void *ret;\n        ret = _kmalloc(size, true, &real_size);\n        return ret;\n}\n\nvoid *kzalloc(size_t size)\n{\n        void *addr;\n\n        addr = kmalloc(size);\n        if (!addr)\n                return NULL;\n        memset(addr, 0, size);\n        return addr;\n}\n\nvoid _kfree(void *ptr, bool is_revoke_record)\n{\n        struct page *page;\n\n        if (unlikely(ptr == ZERO_SIZE_PTR))\n                return;\n\n        page = virt_to_page(ptr);\n#if ENABLE_MEMORY_USAGE_COLLECTING == ON\n        if (collecting_switch && is_revoke_record) {\n                revoke_mem_usage(ptr);\n\t}\n#endif\n        if (page && page->slab)\n                free_in_slab(ptr);\n        else if (page && page->pool)\n                buddy_free_pages(page->pool, page);\n        else\n                kwarn(\"unexpected state in %s\\n\", __func__);\n}\n\nvoid kfree(void *ptr)\n{\n        _kfree(ptr, true);\n}\n"
        }
    ]
}