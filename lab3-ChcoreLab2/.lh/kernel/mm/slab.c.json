{
    "sourceFile": "kernel/mm/slab.c",
    "activeCommit": 0,
    "commits": [
        {
            "activePatchIndex": 0,
            "patches": [
                {
                    "date": 1699241287310,
                    "content": "Index: \n===================================================================\n--- \n+++ \n"
                }
            ],
            "date": 1699241287310,
            "name": "Commit-0",
            "content": "/*\n * Copyright (c) 2023 Institute of Parallel And Distributed Systems (IPADS), Shanghai Jiao Tong University (SJTU)\n * Licensed under the Mulan PSL v2.\n * You can use this software according to the terms and conditions of the Mulan PSL v2.\n * You may obtain a copy of Mulan PSL v2 at:\n *     http://license.coscl.org.cn/MulanPSL2\n * THIS SOFTWARE IS PROVIDED ON AN \"AS IS\" BASIS, WITHOUT WARRANTIES OF ANY KIND, EITHER EXPRESS OR\n * IMPLIED, INCLUDING BUT NOT LIMITED TO NON-INFRINGEMENT, MERCHANTABILITY OR FIT FOR A PARTICULAR\n * PURPOSE.\n * See the Mulan PSL v2 for more details.\n */\n\n#include <common/macro.h>\n#include <common/types.h>\n#include <common/kprint.h>\n#include <common/lock.h>\n#include <common/debug.h>\n#include <mm/kmalloc.h>\n#include <mm/slab.h>\n#include <mm/buddy.h>\n\n/* slab_pool is also static. We do not add the static modifier due to unit test.\n */\nstruct slab_pointer slab_pool[SLAB_MAX_ORDER + 1];\nstatic struct lock slabs_locks[SLAB_MAX_ORDER + 1];\n\n/*\nstatic inline int order_to_index(int order)\n{\n        return order - SLAB_MIN_ORDER;\n}\n*/\n\nstatic inline int size_to_order(unsigned long size)\n{\n        unsigned long order = 0;\n        unsigned long tmp = size;\n\n        while (tmp > 1) {\n                tmp >>= 1;\n                order += 1;\n        }\n        if (size > (1 << order))\n                order += 1;\n\n        return (int)order;\n}\n\nstatic inline unsigned long order_to_size(int order)\n{\n        return 1UL << order;\n}\n\n/* @set_or_clear: true for set and false for clear. */\nstatic void set_or_clear_slab_in_page(void *addr, unsigned long size,\n                                      bool set_or_clear)\n{\n        struct page *page;\n        int order;\n        unsigned long page_num;\n        int i;\n        void *page_addr;\n\n        order = size_to_order(size / BUDDY_PAGE_SIZE); /* need how many BUDDY_PAGE, then calculate order */\n        page_num = order_to_size(order);\n\n        /* Set/Clear the `slab` field in each struct page. */\n        for (i = 0; i < page_num; i++) {\n                page_addr = (void *)((unsigned long)addr + i * BUDDY_PAGE_SIZE);\n                page = virt_to_page(page_addr); \n                if (!page) {\n                        kdebug(\"invalid page in %s\", __func__);\n                        return;\n                }\n                if (set_or_clear)\n                        page->slab = addr;\n                else\n                        page->slab = NULL;\n        }\n}\n\nstatic void *alloc_slab_memory(unsigned long size)\n{\n        void *addr;\n        int order;\n\n        /* Allocate memory pages from the buddy system as a new slab. */\n        order = size_to_order(size / BUDDY_PAGE_SIZE);\n        addr = _get_pages(order, false);\n\n        if (unlikely(addr == NULL)) {\n                kwarn(\"%s failed due to out of memory\\n\", __func__);\n                return NULL;\n        }\n\n        set_or_clear_slab_in_page(addr, size, true);\n\n        return addr;\n}\n\nstatic struct slab_header *init_slab_cache(int order, int size)\n{\n        void *addr;\n        struct slab_slot_list *slot;\n        struct slab_header *slab;\n        unsigned long cnt, obj_size;\n        int i;\n\n        addr = alloc_slab_memory(size);\n        if (unlikely(addr == NULL))\n                /* Fail: no available memory. */\n                return NULL;\n        slab = (struct slab_header *)addr;\n\n        obj_size = order_to_size(order);\n        /* The first slot is used as metadata (struct slab_header). */\n        BUG_ON(obj_size == 0);\n        cnt = size / obj_size - 1;\n\n        slot = (struct slab_slot_list *)((vaddr_t)addr + obj_size);\n        slab->free_list_head = (void *)slot;\n        slab->order = order;\n        slab->total_free_cnt = cnt;\n        slab->current_free_cnt = cnt;\n\n        /* The last slot has no next one. */\n        for (i = 0; i < cnt - 1; ++i) {\n                slot->next_free = (void *)((unsigned long)slot + obj_size);\n                slot = (struct slab_slot_list *)((unsigned long)slot\n                                                 + obj_size);\n        }\n        slot->next_free = NULL;\n\n        return slab;\n}\n\nstatic void choose_new_current_slab(struct slab_pointer *pool, int order)\n{\n        /* LAB 2 TODO 2 BEGIN */\n        /* Hint: Choose a partial slab to be a new current slab. */\n        /* BLANK BEGIN */\n        if (list_empty(&pool -> partial_slab_list)){\n        \tpool -> current_slab = init_slab_cache(order, SIZE_OF_ONE_SLAB); // no available slab, allocate new one\n        } else {\n        pool -> current_slab = list_entry(pool -> partial_slab_list.next, struct slab_header, node);  /* choose the head->next in the partial_slab_list */\n        list_del(pool -> partial_slab_list.next); \n        }\n        \n\n        /* BLANK END */\n        /* LAB 2 TODO 2 END */\n}\n\nstatic void *alloc_in_slab_impl(int order)\n{\n        struct slab_header *current_slab;\n        struct slab_slot_list *free_list;\n        void *next_slot;\n\n        lock(&slabs_locks[order]);\n\n        current_slab = slab_pool[order].current_slab;\n        /* When serving the first allocation request. */\n        /* unlikely is used to optimize branch, which tells compiler this case is unlikely to happen */\n        if (unlikely(current_slab == NULL)) {\n                current_slab = init_slab_cache(order, SIZE_OF_ONE_SLAB);\n                if (current_slab == NULL) {\n                        unlock(&slabs_locks[order]);\n                        return NULL;\n                }\n                slab_pool[order].current_slab = current_slab;\n        }\n\n        /* LAB 2 TODO 2 BEGIN */\n        /*\n         * Hint: Find a free slot from the free list of current slab.\n         * If current slab is full, choose a new slab as the current one.\n         */\n        /* BLANK BEGIN */\n      \tif (current_slab->current_free_cnt == 0) {\n                choose_new_current_slab(&slab_pool[order], order);\n        }\n        free_list = current_slab->free_list_head;\n        current_slab->free_list_head = ((struct slab_slot_list *)(current_slab->free_list_head))->next_free;\n        // free_list->next_free = NULL;\n        --current_slab->current_free_cnt;\n        \n        /* BLANK END */\n        /* LAB 2 TODO 2 END */\n\n        unlock(&slabs_locks[order]);\n\n        return (void *)free_list;\n}\n\n#if ENABLE_DETECTING_DOUBLE_FREE_IN_SLAB == ON\nstatic int check_slot_is_free(struct slab_header *slab_header,\n                              struct slab_slot_list *slot)\n{\n        struct slab_slot_list *cur_slot;\n        struct slab_header *next_slab;\n\n        cur_slot = (struct slab_slot_list *)(slab_header->free_list_head);\n\n        while (cur_slot != NULL) {\n                if (cur_slot == slot)\n                        return 1;\n                cur_slot = (struct slab_slot_list *)cur_slot->next_free;\n        }\n\n        return 0;\n}\n#endif\n\nstatic void try_insert_full_slab_to_partial(struct slab_header *slab)\n{\n        /* @slab is not a full one. */\n        if (slab->current_free_cnt != 0)\n                return;\n\n        int order;\n        order = slab->order;\n\n        list_append(&slab->node, &slab_pool[order].partial_slab_list);\n}\n\nstatic void try_return_slab_to_buddy(struct slab_header *slab, int order)\n{\n        /* The slab is whole free now. */\n        if (slab->current_free_cnt != slab->total_free_cnt)\n                return;\n\n        if (slab == slab_pool[order].current_slab)\n                choose_new_current_slab(&slab_pool[order], order); /* The slab is at current pointer, choose a new one and set it to current*/\n        else\n                list_del(&slab->node); /* The slab is in the partial pointer list, delete it from*/\n\n        /* Clear the slab field in the page structures before freeing them. */\n        set_or_clear_slab_in_page(slab, SIZE_OF_ONE_SLAB, false);\n        free_pages_without_record(slab);\n}\n\n/* Interfaces exported to the kernel/mm moudule */\n\nvoid init_slab(void)\n{\n        int order;\n\n        /* slab obj size: 32, 64, 128, 256, 512, 1024, 2048 */\n        for (order = SLAB_MIN_ORDER; order <= SLAB_MAX_ORDER; order++) {\n                lock_init(&slabs_locks[order]);\n                slab_pool[order].current_slab = NULL;\n                init_list_head(&(slab_pool[order].partial_slab_list));\n        }\n        kdebug(\"mm: finish initing slab allocators\\n\");\n}\n\nvoid *alloc_in_slab(unsigned long size, size_t *real_size)\n{\n        int order;\n\n        BUG_ON(size > order_to_size(SLAB_MAX_ORDER));\n\n        order = (int)size_to_order(size);\n        if (order < SLAB_MIN_ORDER)\n                order = SLAB_MIN_ORDER;\n\n#if ENABLE_MEMORY_USAGE_COLLECTING == ON\n        if (real_size)\n                *real_size = 1 << order;\n#endif\n\n        return alloc_in_slab_impl(order);\n}\n\nvoid free_in_slab(void *addr)\n{\n        struct page *page;\n        struct slab_header *slab;\n        struct slab_slot_list *slot;\n        int order;\n\n        slot = (struct slab_slot_list *)addr;\n        page = virt_to_page(addr);\n        if (!page) {\n                kdebug(\"invalid page in %s\", __func__);\n                return;\n        }\n\n\n        slab = page->slab;\n        order = slab->order;\n        lock(&slabs_locks[order]);\n\n        try_insert_full_slab_to_partial(slab);\n\n#if ENABLE_DETECTING_DOUBLE_FREE_IN_SLAB == ON\n        /*\n         * SLAB double free detection: check whether the slot to free is\n         * already in the free list.\n         */\n        if (check_slot_is_free(slab, slot) == 1) {\n                kinfo(\"SLAB: double free detected. Address is %p\\n\",\n                      (unsigned long)slot);\n                BUG_ON(1);\n        }\n#endif\n\n        /* LAB 2 TODO 2 BEGIN */\n        /*\n         * Hint: Free an allocated slot and put it back to the free list.\n         */\n        /* BLANK BEGIN */\n\tslot -> next_free = slab -> free_list_head;\n\tslab -> free_list_head = (void *)slot;\n\t++ slab -> current_free_cnt;\n        /* BLANK END */\n        /* LAB 2 TODO 2 END */\n\n        try_return_slab_to_buddy(slab, order);\n\n        unlock(&slabs_locks[order]);\n}\n\n/* This interface is not marked as static because it is needed in the unit test.\n */\nunsigned long get_free_slot_number(int order)\n{\n        struct slab_header *slab;\n        struct slab_slot_list *slot;\n        unsigned long current_slot_num = 0;\n        unsigned long check_slot_num = 0;\n\n        lock(&slabs_locks[order]);\n\n        slab = slab_pool[order].current_slab;\n        if (slab) {\n                slot = (struct slab_slot_list *)slab->free_list_head;\n                while (slot != NULL) {\n                        current_slot_num++;\n                        slot = slot->next_free;\n                }\n                check_slot_num += slab->current_free_cnt;\n        }\n\n        if (list_empty(&slab_pool[order].partial_slab_list))\n                goto out;\n\n        for_each_in_list (slab,\n                          struct slab_header,\n                          node,\n                          &slab_pool[order].partial_slab_list) {\n                slot = (struct slab_slot_list *)slab->free_list_head;\n                while (slot != NULL) {\n                        current_slot_num++;\n                        slot = slot->next_free;\n                }\n                check_slot_num += slab->current_free_cnt;\n        }\n\nout:\n        unlock(&slabs_locks[order]);\n\n        BUG_ON(check_slot_num != current_slot_num);\n\n        return current_slot_num;\n}\n\n/* Get the size of free memory in slab */\nunsigned long get_free_mem_size_from_slab(void)\n{\n        int order;\n        unsigned long current_slot_size;\n        unsigned long slot_num;\n        unsigned long total_size = 0;\n\n        for (order = SLAB_MIN_ORDER; order <= SLAB_MAX_ORDER; order++) {\n                current_slot_size = order_to_size(order);\n                slot_num = get_free_slot_number(order);\n                total_size += (current_slot_size * slot_num);\n\n                kdebug(\"slab memory chunk size : 0x%lx, num : %d\\n\",\n                       current_slot_size,\n                       slot_num);\n        }\n\n        return total_size;\n}\n"
        }
    ]
}